{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome The Advection Diffusion Equation Post Large Eddy Simulation (adepostles) code solves tracer transport as a post processing step to LES simulations. It was initially developped to treat output from DALES , but the idea is to be able to post process a broad set of LES outputs, as long as they conform to NetCDF. Features IBM MPI parallelization NetCDF input/output format NOTE: The parallelization goal is to allow multiple initial conditions and source profiles to be calculated in parallel for the same LES simulation output","title":"Welcome"},{"location":"#welcome","text":"The Advection Diffusion Equation Post Large Eddy Simulation (adepostles) code solves tracer transport as a post processing step to LES simulations. It was initially developped to treat output from DALES , but the idea is to be able to post process a broad set of LES outputs, as long as they conform to NetCDF.","title":"Welcome"},{"location":"#features","text":"IBM MPI parallelization NetCDF input/output format NOTE: The parallelization goal is to allow multiple initial conditions and source profiles to be calculated in parallel for the same LES simulation output","title":"Features"},{"location":"installation/","text":"Installation Let us run throught the installation of ADEpostLES Requirements Fortran compiler : recomended gfortran (tested with 11 and 13) Cmake : recomended to just download and install the binary MPI : tested with Open MPI > 4.1.4. Installation guides: 1 , 2 NetCDF : Netcdf fortran, recomended installation using conda NOTE You might need to export the paths to your netcdf-fortran instalations, you can find them using nf-config . See the following example, cmake compilation first looks for these environment variables. export NETCDF_INCLUDE=/home/user/miniconda3/envs/fort/include export NETCDF_LIB=/home/user/miniconda3/envs/fort/lib NOTE When compiling in SLRUM environments like DelftBlue, the following commands are enough to meet requirements: module load 2023r1-gcc11 module load openmpi/4.1.4 module load cmake/3.24.3 module load netcdf-fortran/4.6.0 Compilation Clone the github repo git clone https://github.com/lorenzodonadio/adepostles.git Create a build directory, run Cmake and compile with make cd adepostles mkdir build cd build cmake .. make Compilations is finished, the executable should be located at: /adepostles/build/bin/main","title":"Installation"},{"location":"installation/#installation","text":"Let us run throught the installation of ADEpostLES","title":"Installation"},{"location":"installation/#requirements","text":"Fortran compiler : recomended gfortran (tested with 11 and 13) Cmake : recomended to just download and install the binary MPI : tested with Open MPI > 4.1.4. Installation guides: 1 , 2 NetCDF : Netcdf fortran, recomended installation using conda NOTE You might need to export the paths to your netcdf-fortran instalations, you can find them using nf-config . See the following example, cmake compilation first looks for these environment variables. export NETCDF_INCLUDE=/home/user/miniconda3/envs/fort/include export NETCDF_LIB=/home/user/miniconda3/envs/fort/lib NOTE When compiling in SLRUM environments like DelftBlue, the following commands are enough to meet requirements: module load 2023r1-gcc11 module load openmpi/4.1.4 module load cmake/3.24.3 module load netcdf-fortran/4.6.0","title":"Requirements"},{"location":"installation/#compilation","text":"Clone the github repo git clone https://github.com/lorenzodonadio/adepostles.git Create a build directory, run Cmake and compile with make cd adepostles mkdir build cd build cmake .. make Compilations is finished, the executable should be located at: /adepostles/build/bin/main","title":"Compilation"},{"location":"namoptions/","text":"Config file sample: Sample namoptions file: &RUN runtime = 700 dtmax = 1 ladaptivedt = .true. lanisotrop = .true. rkmethod = 2 field_load_chunk_size = 2 output_save_interval = 5 courant_limit = 0.8 vonneumann_limit = 2 field_dump_path = \"fielddump_30s.nc\" sources_prefix = \"tracer_inp/tracer_inp_001_001.nc\" outputfile_path = \"must_conc_30s_periodictest.nc\" / &BOUNDARY xboundary = 12 yboundary = 12 / &IBM lapplyibm = .true. ibm_input_file = \"ibm.inp.001\" / RUN Section Parameters controlling the core runtime behavior and input/output settings. Time Control runtime : duration of the simulation is seconds dtmax : max timestep of the simulation in seconds ladaptivedt : (optional default true) switch to use adaptive timestep, if tuned off then dtmax is the timestep for the simulation. courant_limit : (optional) CFL limit, adviced around 0.3 for complex geometries (defaults depend on RK scheme) vonneumann_limit : (optional) von neumann limit, usually not the limiting factor for the timestep but adviced around 1 (defaults depend on RK scheme), output_save_interval : number of seconds between result saves (default 20) Simulation Parameters lanisotrop : (default true), use anisotrophic diffusion scheme (moddiff.f90) rkmethod : whick Runge-Kutta method to use: 1. euler, 2.heun, 3.rk3, 4. rk4. (default 1) field_load_chunk_size : integer that controls how many time steps of the wind field are loaded; how big is the chunk. The larger it is, the faster the simulation (less i/o) but the more memory requirements. File Configuration field_dump_path : path for the LES output .nc file ( example ) sources_prefix : file or prefix of the tracer source .nc file ( example ) outputfile_path : path where the results will be saved. This filename will be suffixed with .001, .002, etc for the processor number in case parallel simulations are run. BOUNDARY Section possible values: 11: Neumann 1st Order, 12: Neumann 2nd Order, 2: Periodic. It is not recomended to use 1st order BC xboundary : boundary condition for east-west direction yboundary : boundary condition for north-south direction IBM Section Immersed Boundary Method configuration. lapplyibm : boolean to use ibm or not ibm_input_file : path to the ibm file","title":"Configuration"},{"location":"namoptions/#config-file-sample","text":"Sample namoptions file: &RUN runtime = 700 dtmax = 1 ladaptivedt = .true. lanisotrop = .true. rkmethod = 2 field_load_chunk_size = 2 output_save_interval = 5 courant_limit = 0.8 vonneumann_limit = 2 field_dump_path = \"fielddump_30s.nc\" sources_prefix = \"tracer_inp/tracer_inp_001_001.nc\" outputfile_path = \"must_conc_30s_periodictest.nc\" / &BOUNDARY xboundary = 12 yboundary = 12 / &IBM lapplyibm = .true. ibm_input_file = \"ibm.inp.001\" /","title":"Config file sample:"},{"location":"namoptions/#run-section","text":"Parameters controlling the core runtime behavior and input/output settings.","title":"RUN Section"},{"location":"namoptions/#time-control","text":"runtime : duration of the simulation is seconds dtmax : max timestep of the simulation in seconds ladaptivedt : (optional default true) switch to use adaptive timestep, if tuned off then dtmax is the timestep for the simulation. courant_limit : (optional) CFL limit, adviced around 0.3 for complex geometries (defaults depend on RK scheme) vonneumann_limit : (optional) von neumann limit, usually not the limiting factor for the timestep but adviced around 1 (defaults depend on RK scheme), output_save_interval : number of seconds between result saves (default 20)","title":"Time Control"},{"location":"namoptions/#simulation-parameters","text":"lanisotrop : (default true), use anisotrophic diffusion scheme (moddiff.f90) rkmethod : whick Runge-Kutta method to use: 1. euler, 2.heun, 3.rk3, 4. rk4. (default 1) field_load_chunk_size : integer that controls how many time steps of the wind field are loaded; how big is the chunk. The larger it is, the faster the simulation (less i/o) but the more memory requirements.","title":"Simulation Parameters"},{"location":"namoptions/#file-configuration","text":"field_dump_path : path for the LES output .nc file ( example ) sources_prefix : file or prefix of the tracer source .nc file ( example ) outputfile_path : path where the results will be saved. This filename will be suffixed with .001, .002, etc for the processor number in case parallel simulations are run.","title":"File Configuration"},{"location":"namoptions/#boundary-section","text":"possible values: 11: Neumann 1st Order, 12: Neumann 2nd Order, 2: Periodic. It is not recomended to use 1st order BC xboundary : boundary condition for east-west direction yboundary : boundary condition for north-south direction","title":"BOUNDARY Section"},{"location":"namoptions/#ibm-section","text":"Immersed Boundary Method configuration. lapplyibm : boolean to use ibm or not ibm_input_file : path to the ibm file","title":"IBM Section"},{"location":"run/","text":"Run After you have compiled Adepostles, you are almost ready to run your first simulation. It is adviced to copy the main executable to a separate directory where you want to run the simulation not to clutter build/bin Input files The executable takes only one config file as input, this is usually called namoptions in DALES. but can have any name. A complete description of namoptions can be found here . Within namoptions we must specify 3 file paths: 1. field_dump_path path to the fielddump, the output of the LES simulation merged into one single NetCDF file . The file must contain: time dimension x,y,z coordinates of the domain 4D fields (time,z,y,x): wind velocity (u,v,w), eddy viscosity (ekh) 2D vertical profiles (time, z) of density: rhobf 2. sources_prefix sources_prefix specifies the source file for the concentration, which has both initial conditions and time varying sources. For runs with one single core then source_prefix must be exactly the path to the source file and end with .nc For runs in parallel ,i.e with multiple source files, then only the prefix to the NetCDF file should be provided, and the program will automatically append _001.nc for the first core and _002.nc for the second core and so on. So you must prepare source files that end exactly that way up to the number of cores you are using for the run. 3. ibm_input_file The IBM file is not NetCDF only because DALES still takes a text file as input for the IBM and it is important that the same file use for IBM in the LES is used for Adepostles. Example files field_dump Here is an example of the result of ncdump -h of a fielddump with the expected format: dimensions: time = 1480 ; zt = 20 ; zm = 20 ; xt = 128 ; xm = 128 ; yt = 128 ; ym = 128 ; variables: float time(time) ; float zt(zt) ; float zm(zm) ; float xt(xt) ; float xm(xm) ; float yt(yt) ; float ym(ym) ; float u(time, zt, yt, xm) ; float v(time, zt, ym, xt) ; float w(time, zm, yt, xt) ; float ekh(time, zt, yt, xt) ; float rhof(time, zt) ; float rhobf(time, zt) ; float rhobh(time, zm) ; float presh(time, zt) ; sources Example of netcdf structure for a source file, that has 2 separate groups (set of tracers) each with only one source init is left empty so it means 0 everywhere one could imagine multiple sources per tracer some utilities to create these source files: TODO insert link netcdf tracer_inp_001_001 { dimensions: time = 10 ; x = 256 ; y = 256 ; z = 40 ; // global attributes: :description = \"NetCDF file for tracer properties\" ; :history = \"Created with T_tracer structure equivalent\" ; :numtracers = 2LL ; :totnumsources = 2LL ; group: s0 { variables: float init(z, y, x) ; float source_1(time) ; source_1:x = 8LL ; source_1:y = 30LL ; source_1:z = 2LL ; // group attributes: :tracname = \"s0\" ; :traclong = \"s0_x_8_y_30\" ; :unit = \"kg/kg\" ; :molar_mass = -999. ; :lemis = \"true\" ; :numsources = 1LL ; } // group s0 group: s1 { variables: float init(z, y, x) ; float source_1(time) ; source_1:x = 8LL ; source_1:y = 45LL ; source_1:z = 2LL ; // group attributes: :tracname = \"s1\" ; :traclong = \"s1_x_8_y_45\" ; :unit = \"kg/kg\" ; :molar_mass = -999. ; :lemis = \"true\" ; :numsources = 1LL ; } // group s1 }","title":"Run"},{"location":"run/#run","text":"After you have compiled Adepostles, you are almost ready to run your first simulation. It is adviced to copy the main executable to a separate directory where you want to run the simulation not to clutter build/bin","title":"Run"},{"location":"run/#input-files","text":"The executable takes only one config file as input, this is usually called namoptions in DALES. but can have any name. A complete description of namoptions can be found here . Within namoptions we must specify 3 file paths: 1. field_dump_path path to the fielddump, the output of the LES simulation merged into one single NetCDF file . The file must contain: time dimension x,y,z coordinates of the domain 4D fields (time,z,y,x): wind velocity (u,v,w), eddy viscosity (ekh) 2D vertical profiles (time, z) of density: rhobf 2. sources_prefix sources_prefix specifies the source file for the concentration, which has both initial conditions and time varying sources. For runs with one single core then source_prefix must be exactly the path to the source file and end with .nc For runs in parallel ,i.e with multiple source files, then only the prefix to the NetCDF file should be provided, and the program will automatically append _001.nc for the first core and _002.nc for the second core and so on. So you must prepare source files that end exactly that way up to the number of cores you are using for the run. 3. ibm_input_file The IBM file is not NetCDF only because DALES still takes a text file as input for the IBM and it is important that the same file use for IBM in the LES is used for Adepostles.","title":"Input files"},{"location":"run/#example-files","text":"field_dump Here is an example of the result of ncdump -h of a fielddump with the expected format: dimensions: time = 1480 ; zt = 20 ; zm = 20 ; xt = 128 ; xm = 128 ; yt = 128 ; ym = 128 ; variables: float time(time) ; float zt(zt) ; float zm(zm) ; float xt(xt) ; float xm(xm) ; float yt(yt) ; float ym(ym) ; float u(time, zt, yt, xm) ; float v(time, zt, ym, xt) ; float w(time, zm, yt, xt) ; float ekh(time, zt, yt, xt) ; float rhof(time, zt) ; float rhobf(time, zt) ; float rhobh(time, zm) ; float presh(time, zt) ; sources Example of netcdf structure for a source file, that has 2 separate groups (set of tracers) each with only one source init is left empty so it means 0 everywhere one could imagine multiple sources per tracer some utilities to create these source files: TODO insert link netcdf tracer_inp_001_001 { dimensions: time = 10 ; x = 256 ; y = 256 ; z = 40 ; // global attributes: :description = \"NetCDF file for tracer properties\" ; :history = \"Created with T_tracer structure equivalent\" ; :numtracers = 2LL ; :totnumsources = 2LL ; group: s0 { variables: float init(z, y, x) ; float source_1(time) ; source_1:x = 8LL ; source_1:y = 30LL ; source_1:z = 2LL ; // group attributes: :tracname = \"s0\" ; :traclong = \"s0_x_8_y_30\" ; :unit = \"kg/kg\" ; :molar_mass = -999. ; :lemis = \"true\" ; :numsources = 1LL ; } // group s0 group: s1 { variables: float init(z, y, x) ; float source_1(time) ; source_1:x = 8LL ; source_1:y = 45LL ; source_1:z = 2LL ; // group attributes: :tracname = \"s1\" ; :traclong = \"s1_x_8_y_45\" ; :unit = \"kg/kg\" ; :molar_mass = -999. ; :lemis = \"true\" ; :numsources = 1LL ; } // group s1 }","title":"Example files"},{"location":"utilities/","text":"Utilities Utilities to handle NetCDF output from DALES and also to generate source profiles and prepare the run of DALES and ADEpostLES: Source code [here] (https://github.com/lorenzodonadio/les-utils)","title":"Utilities"},{"location":"utilities/#utilities","text":"Utilities to handle NetCDF output from DALES and also to generate source profiles and prepare the run of DALES and ADEpostLES: Source code [here] (https://github.com/lorenzodonadio/les-utils)","title":"Utilities"},{"location":"utilities/gensources/","text":"Tracer Generation Utility for NetCDF Files Overview This utility is designed to generate tracer source profiles for atmospheric or environmental modeling, specifically creating NetCDF files with point source emissions across multiple spatial and temporal dimensions. Components 1. sourcegen.py A Python module that provides core functionality for creating NetCDF tracer profiles: - Creates NetCDF files with tracer information - Supports adding multiple tracers - Allows definition of point sources for each tracer - Manages dimensions and metadata for tracer profiles 2. must_tracergen.py (example) The main script for generating tracer input files, modify this to match your needs : - Generates multiple tracer profiles based on predefined configurations - Supports parallel processing with configurable parameters - Creates multiple NetCDF files with point source emissions 3. gensource.sh A shell script for running the tracer generation on a high-performance computing (HPC) environment: - Uses SLURM job scheduling - Configures environment and dependencies - Executes the tracer generation script Configuration Parameters Key configuration parameters in must_tracergen.py : - tracers_per_core : Number of tracers per computational core (default: 3) - nproc : Number of processors (default: 2) - nruns : Number of simulation runs (default: 11) - z_idx_src : Vertical index for point sources (default: 2) - step_src_mat : Source profile matrix defining emission values over time - point_sources : Array of point source coordinates Usage Prerequisites Python 3.x Required libraries: numpy netCDF4 tqdm Running the Script python must_tracergen.py <fielddump_netcdf_path> --output_dir <output_directory> Example: python must_tracergen.py fielddump_10s.nc --output_dir ./tracers/ HPC Deployment The provided gensource.sh script can be used to submit the job to a SLURM-managed cluster: sbatch gensource.sh Output The script generates multiple NetCDF files in the specified output directory: - Filename format: tracer_inp_XXX_YYY.nc - XXX : Run number (001-011) - YYY : Processor number (001-002) Each file contains: - Tracer metadata - Point source emission profiles - Spatial and temporal emission information Customization To modify the tracer generation: 1. Adjust point_sources array to change source locations 2. Modify step_src_mat to alter emission profiles 3. Change tracers_per_core , nproc , and nruns to match your simulation requirements Note This utility is specifically designed for atmospheric or environmental modeling workflows that require detailed tracer source definition across multiple dimensions.","title":"Generate Sources"},{"location":"utilities/gensources/#tracer-generation-utility-for-netcdf-files","text":"","title":"Tracer Generation Utility for NetCDF Files"},{"location":"utilities/gensources/#overview","text":"This utility is designed to generate tracer source profiles for atmospheric or environmental modeling, specifically creating NetCDF files with point source emissions across multiple spatial and temporal dimensions.","title":"Overview"},{"location":"utilities/gensources/#components","text":"","title":"Components"},{"location":"utilities/gensources/#1-sourcegenpy","text":"A Python module that provides core functionality for creating NetCDF tracer profiles: - Creates NetCDF files with tracer information - Supports adding multiple tracers - Allows definition of point sources for each tracer - Manages dimensions and metadata for tracer profiles","title":"1. sourcegen.py"},{"location":"utilities/gensources/#2-must_tracergenpy-example","text":"The main script for generating tracer input files, modify this to match your needs : - Generates multiple tracer profiles based on predefined configurations - Supports parallel processing with configurable parameters - Creates multiple NetCDF files with point source emissions","title":"2. must_tracergen.py (example)"},{"location":"utilities/gensources/#3-gensourcesh","text":"A shell script for running the tracer generation on a high-performance computing (HPC) environment: - Uses SLURM job scheduling - Configures environment and dependencies - Executes the tracer generation script","title":"3. gensource.sh"},{"location":"utilities/gensources/#configuration-parameters","text":"Key configuration parameters in must_tracergen.py : - tracers_per_core : Number of tracers per computational core (default: 3) - nproc : Number of processors (default: 2) - nruns : Number of simulation runs (default: 11) - z_idx_src : Vertical index for point sources (default: 2) - step_src_mat : Source profile matrix defining emission values over time - point_sources : Array of point source coordinates","title":"Configuration Parameters"},{"location":"utilities/gensources/#usage","text":"","title":"Usage"},{"location":"utilities/gensources/#prerequisites","text":"Python 3.x Required libraries: numpy netCDF4 tqdm","title":"Prerequisites"},{"location":"utilities/gensources/#running-the-script","text":"python must_tracergen.py <fielddump_netcdf_path> --output_dir <output_directory> Example: python must_tracergen.py fielddump_10s.nc --output_dir ./tracers/","title":"Running the Script"},{"location":"utilities/gensources/#hpc-deployment","text":"The provided gensource.sh script can be used to submit the job to a SLURM-managed cluster: sbatch gensource.sh","title":"HPC Deployment"},{"location":"utilities/gensources/#output","text":"The script generates multiple NetCDF files in the specified output directory: - Filename format: tracer_inp_XXX_YYY.nc - XXX : Run number (001-011) - YYY : Processor number (001-002) Each file contains: - Tracer metadata - Point source emission profiles - Spatial and temporal emission information","title":"Output"},{"location":"utilities/gensources/#customization","text":"To modify the tracer generation: 1. Adjust point_sources array to change source locations 2. Modify step_src_mat to alter emission profiles 3. Change tracers_per_core , nproc , and nruns to match your simulation requirements","title":"Customization"},{"location":"utilities/gensources/#note","text":"This utility is specifically designed for atmospheric or environmental modeling workflows that require detailed tracer source definition across multiple dimensions.","title":"Note"},{"location":"utilities/postprocesdales/","text":"NetCDF Field Merger and Profile Adder Overview This utility provides a comprehensive solution for merging distributed NetCDF files from parallel computational simulations and adding profile variables to the merged dataset. Components 1. fieldmerge.py A Python script with two primary functionalities: - Merging distributed NetCDF files from multiple processing tiles - Adding profile variables to an existing NetCDF dataset 2. merge.sh A shell script for submitting the field merging job to a SLURM-managed high-performance computing (HPC) environment. Key Classes and Functions PostProcessField Class Manages the merging of NetCDF files from multiple processing tiles. Methods __init__(file_path) : Initializes the merger by discovering and organizing NetCDF files extract_field(var_name, time_start, time_end) : Extracts variable data across all tiles for a specified time range save_to_single_netcdf(output_file, chunk_size) : Merges all discovered NetCDF files into a single file Main Functions add_profiles_to_dataset(profile_path, dataset_path) : Adds profile variables to an existing NetCDF file Usage Command-line Interface Two primary sub-commands are available: Merge NetCDF Files python fieldmerge.py merge --input_dir /path/to/netcdf/files \\ --output_file completefielddump.nc \\ --chunk_size 50 \\ --profile_file optional_profiles.nc Add Profiles to Existing Dataset python fieldmerge.py add_profiles --profile_file profiles.nc \\ --dataset_file existing_dataset.nc HPC Deployment The merge.sh script provides a SLURM job configuration for running the merge process: sbatch merge.sh Configuration Parameters Merge Command --input_dir : Directory containing distributed NetCDF files --output_file : Path for the merged output file --chunk_size : Number of time steps to process in each iteration (default: 50) --profile_file : Optional profile file to merge with the dataset Add Profiles Command --profile_file : Path to the profile NetCDF file --dataset_file : Path to the existing NetCDF dataset Requirements Python 3.x Libraries: numpy netCDF4 tqdm Limitations and Considerations Assumes a specific file naming convention for distributed NetCDF files Requires matching time dimensions between profile and main dataset Memory usage depends on chunk_size and total dataset size Notes Supports merging files from multiple processing tiles Allows optional addition of profile variables post-merging Example Workflow Run parallel simulation generating multiple NetCDF files Use fieldmerge.py merge to combine files into a single dataset Optionally add profile variables using the same script NetCDF Field Subsampling Utility Overview This utility provides a flexible tool for subsampling large NetCDF files, allowing users to extract time-series data at different sampling rates while preserving the original dataset's structure and attributes. Components 1. fieldsubsample.py A Python script for subsampling NetCDF files with the following key features: - Extracting time series data at specified sampling intervals - Skipping initial non-physical time steps - Batch processing of large datasets - Preserving original variable attributes and global metadata 2. subsample.sh A SLURM job script for submitting the field subsampling process to a high-performance computing (HPC) environment. Key Functions subsample_netcdf() Primary function for subsampling NetCDF files Parameters input_file (str): Path to the source NetCDF file output_dir (str, optional): Directory to save subsampled files (default: current directory) skip_first (int, optional): Number of initial time steps to skip (default: 120) sampling_rates (list, optional): List of sampling rates in seconds (default: [5, 10, 30, 60]) nbatches (int, optional): Number of batches to process data (default: 10) Usage Command-line Interface python fieldsubsample.py input_file.nc \\ --output_dir ./output \\ --skip_first 432 \\ --sampling_rates 1,5,10,30,60 \\ --nbatches 10 HPC Deployment Use the provided subsample.sh SLURM script: sbatch subsample.sh Configuration Options Command-line Arguments input_file : Required input NetCDF file path --output_dir : Output directory for subsampled files --skip_first : Number of initial time steps to skip --sampling_rates : Comma-separated sampling rates (in seconds) --nbatches : Number of data processing batches Workflow Load the original NetCDF file Skip specified initial time steps Create subsampled files at different sampling rates Preserve original variable attributes and metadata Save output files with rate-specific naming (e.g., fielddump_10s.nc ) Requirements Python 3.x Libraries: numpy netCDF4 tqdm Key Features Flexible sampling rate configuration Batch processing for large datasets Preserves original data structure Handles time-dependent and time-independent variables Supports multiple output files for different sampling intervals Limitations and Considerations Assumes uniform time step in source data Memory usage depends on dataset size and batch configuration Requires selection of skip_first and sampling rates","title":"PostProcess DALES"},{"location":"utilities/postprocesdales/#netcdf-field-merger-and-profile-adder","text":"","title":"NetCDF Field Merger and Profile Adder"},{"location":"utilities/postprocesdales/#overview","text":"This utility provides a comprehensive solution for merging distributed NetCDF files from parallel computational simulations and adding profile variables to the merged dataset.","title":"Overview"},{"location":"utilities/postprocesdales/#components","text":"","title":"Components"},{"location":"utilities/postprocesdales/#1-fieldmergepy","text":"A Python script with two primary functionalities: - Merging distributed NetCDF files from multiple processing tiles - Adding profile variables to an existing NetCDF dataset","title":"1. fieldmerge.py"},{"location":"utilities/postprocesdales/#2-mergesh","text":"A shell script for submitting the field merging job to a SLURM-managed high-performance computing (HPC) environment.","title":"2. merge.sh"},{"location":"utilities/postprocesdales/#key-classes-and-functions","text":"","title":"Key Classes and Functions"},{"location":"utilities/postprocesdales/#postprocessfield-class","text":"Manages the merging of NetCDF files from multiple processing tiles.","title":"PostProcessField Class"},{"location":"utilities/postprocesdales/#methods","text":"__init__(file_path) : Initializes the merger by discovering and organizing NetCDF files extract_field(var_name, time_start, time_end) : Extracts variable data across all tiles for a specified time range save_to_single_netcdf(output_file, chunk_size) : Merges all discovered NetCDF files into a single file","title":"Methods"},{"location":"utilities/postprocesdales/#main-functions","text":"add_profiles_to_dataset(profile_path, dataset_path) : Adds profile variables to an existing NetCDF file","title":"Main Functions"},{"location":"utilities/postprocesdales/#usage","text":"","title":"Usage"},{"location":"utilities/postprocesdales/#command-line-interface","text":"Two primary sub-commands are available: Merge NetCDF Files python fieldmerge.py merge --input_dir /path/to/netcdf/files \\ --output_file completefielddump.nc \\ --chunk_size 50 \\ --profile_file optional_profiles.nc Add Profiles to Existing Dataset python fieldmerge.py add_profiles --profile_file profiles.nc \\ --dataset_file existing_dataset.nc","title":"Command-line Interface"},{"location":"utilities/postprocesdales/#hpc-deployment","text":"The merge.sh script provides a SLURM job configuration for running the merge process: sbatch merge.sh","title":"HPC Deployment"},{"location":"utilities/postprocesdales/#configuration-parameters","text":"","title":"Configuration Parameters"},{"location":"utilities/postprocesdales/#merge-command","text":"--input_dir : Directory containing distributed NetCDF files --output_file : Path for the merged output file --chunk_size : Number of time steps to process in each iteration (default: 50) --profile_file : Optional profile file to merge with the dataset","title":"Merge Command"},{"location":"utilities/postprocesdales/#add-profiles-command","text":"--profile_file : Path to the profile NetCDF file --dataset_file : Path to the existing NetCDF dataset","title":"Add Profiles Command"},{"location":"utilities/postprocesdales/#requirements","text":"Python 3.x Libraries: numpy netCDF4 tqdm","title":"Requirements"},{"location":"utilities/postprocesdales/#limitations-and-considerations","text":"Assumes a specific file naming convention for distributed NetCDF files Requires matching time dimensions between profile and main dataset Memory usage depends on chunk_size and total dataset size","title":"Limitations and Considerations"},{"location":"utilities/postprocesdales/#notes","text":"Supports merging files from multiple processing tiles Allows optional addition of profile variables post-merging","title":"Notes"},{"location":"utilities/postprocesdales/#example-workflow","text":"Run parallel simulation generating multiple NetCDF files Use fieldmerge.py merge to combine files into a single dataset Optionally add profile variables using the same script","title":"Example Workflow"},{"location":"utilities/postprocesdales/#netcdf-field-subsampling-utility","text":"","title":"NetCDF Field Subsampling Utility"},{"location":"utilities/postprocesdales/#overview_1","text":"This utility provides a flexible tool for subsampling large NetCDF files, allowing users to extract time-series data at different sampling rates while preserving the original dataset's structure and attributes.","title":"Overview"},{"location":"utilities/postprocesdales/#components_1","text":"","title":"Components"},{"location":"utilities/postprocesdales/#1-fieldsubsamplepy","text":"A Python script for subsampling NetCDF files with the following key features: - Extracting time series data at specified sampling intervals - Skipping initial non-physical time steps - Batch processing of large datasets - Preserving original variable attributes and global metadata","title":"1. fieldsubsample.py"},{"location":"utilities/postprocesdales/#2-subsamplesh","text":"A SLURM job script for submitting the field subsampling process to a high-performance computing (HPC) environment.","title":"2. subsample.sh"},{"location":"utilities/postprocesdales/#key-functions","text":"","title":"Key Functions"},{"location":"utilities/postprocesdales/#subsample_netcdf","text":"Primary function for subsampling NetCDF files","title":"subsample_netcdf()"},{"location":"utilities/postprocesdales/#parameters","text":"input_file (str): Path to the source NetCDF file output_dir (str, optional): Directory to save subsampled files (default: current directory) skip_first (int, optional): Number of initial time steps to skip (default: 120) sampling_rates (list, optional): List of sampling rates in seconds (default: [5, 10, 30, 60]) nbatches (int, optional): Number of batches to process data (default: 10)","title":"Parameters"},{"location":"utilities/postprocesdales/#usage_1","text":"","title":"Usage"},{"location":"utilities/postprocesdales/#command-line-interface_1","text":"python fieldsubsample.py input_file.nc \\ --output_dir ./output \\ --skip_first 432 \\ --sampling_rates 1,5,10,30,60 \\ --nbatches 10","title":"Command-line Interface"},{"location":"utilities/postprocesdales/#hpc-deployment_1","text":"Use the provided subsample.sh SLURM script: sbatch subsample.sh","title":"HPC Deployment"},{"location":"utilities/postprocesdales/#configuration-options","text":"","title":"Configuration Options"},{"location":"utilities/postprocesdales/#command-line-arguments","text":"input_file : Required input NetCDF file path --output_dir : Output directory for subsampled files --skip_first : Number of initial time steps to skip --sampling_rates : Comma-separated sampling rates (in seconds) --nbatches : Number of data processing batches","title":"Command-line Arguments"},{"location":"utilities/postprocesdales/#workflow","text":"Load the original NetCDF file Skip specified initial time steps Create subsampled files at different sampling rates Preserve original variable attributes and metadata Save output files with rate-specific naming (e.g., fielddump_10s.nc )","title":"Workflow"},{"location":"utilities/postprocesdales/#requirements_1","text":"Python 3.x Libraries: numpy netCDF4 tqdm","title":"Requirements"},{"location":"utilities/postprocesdales/#key-features","text":"Flexible sampling rate configuration Batch processing for large datasets Preserves original data structure Handles time-dependent and time-independent variables Supports multiple output files for different sampling intervals","title":"Key Features"},{"location":"utilities/postprocesdales/#limitations-and-considerations_1","text":"Assumes uniform time step in source data Memory usage depends on dataset size and batch configuration Requires selection of skip_first and sampling rates","title":"Limitations and Considerations"},{"location":"utilities/subsample/","text":"","title":"Subsample"},{"location":"utilities/visualizations/","text":"How to easily visualize results? WIP","title":"Visualize Results"},{"location":"utilities/visualizations/#how-to-easily-visualize-results","text":"WIP","title":"How to easily visualize results?"}]}